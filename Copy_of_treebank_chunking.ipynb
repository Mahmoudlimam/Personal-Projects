{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahmoudlimam/Personal-Projects/blob/main/Copy_of_treebank_chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6994d50",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:20.120991Z",
          "iopub.status.busy": "2023-07-05T17:45:20.120129Z",
          "iopub.status.idle": "2023-07-05T17:45:22.799307Z",
          "shell.execute_reply": "2023-07-05T17:45:22.797872Z"
        },
        "papermill": {
          "duration": 2.689035,
          "end_time": "2023-07-05T17:45:22.802168",
          "exception": false,
          "start_time": "2023-07-05T17:45:20.113133",
          "status": "completed"
        },
        "tags": [],
        "id": "f6994d50",
        "outputId": "bfe76cbe-b800-4a79-d881-a9922404bfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'quran-neural-chunker'...\r\n",
            "remote: Enumerating objects: 265, done.\u001b[K\r\n",
            "remote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\u001b[K\r\n",
            "Receiving objects: 100% (265/265), 14.54 MiB | 21.54 MiB/s, done.\r\n",
            "Resolving deltas: 100% (144/144), done.\r\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kaisdukes/quran-neural-chunker.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa76012",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:22.811860Z",
          "iopub.status.busy": "2023-07-05T17:45:22.811462Z",
          "iopub.status.idle": "2023-07-05T17:45:23.893553Z",
          "shell.execute_reply": "2023-07-05T17:45:23.891967Z"
        },
        "papermill": {
          "duration": 1.090281,
          "end_time": "2023-07-05T17:45:23.896601",
          "exception": false,
          "start_time": "2023-07-05T17:45:22.806320",
          "status": "completed"
        },
        "tags": [],
        "id": "afa76012"
      },
      "outputs": [],
      "source": [
        "!cd /kaggle/working/quran-neural-chunker/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1b37f9",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:23.907016Z",
          "iopub.status.busy": "2023-07-05T17:45:23.906214Z",
          "iopub.status.idle": "2023-07-05T17:45:28.379894Z",
          "shell.execute_reply": "2023-07-05T17:45:28.378958Z"
        },
        "papermill": {
          "duration": 4.482031,
          "end_time": "2023-07-05T17:45:28.382418",
          "exception": false,
          "start_time": "2023-07-05T17:45:23.900387",
          "status": "completed"
        },
        "tags": [],
        "id": "6d1b37f9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from pandas import DataFrame\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "max_length = 128\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Location:\n",
        "    chapter_number: int\n",
        "    verse_number: int\n",
        "    token_number: int\n",
        "\n",
        "    def __str__(self):\n",
        "        parts = [str(self.chapter_number), str(self.verse_number)]\n",
        "        if self.token_number > 0:\n",
        "            parts.append(str(self.token_number))\n",
        "        return ':'.join(parts)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Chunk:\n",
        "    start: Location\n",
        "    end: Location\n",
        "\n",
        "\n",
        "def get_chunks(df: DataFrame):\n",
        "    chunks: List[Chunk] = []\n",
        "    start: Location = None\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        loc = Location(row['chapter_number'], row['verse_number'], row['token_number'])\n",
        "        if start is None:\n",
        "            start = loc\n",
        "        if row['chunk_end'] == 1:\n",
        "            end = loc\n",
        "            chunk = Chunk(start, end)\n",
        "            chunks.append(chunk)\n",
        "            start = None\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def preprocess(df: DataFrame):\n",
        "    df['verse_end'] = (\n",
        "        (df.groupby(['chapter_number', 'verse_number']).token_number.transform(max) == df.token_number)\n",
        "        .astype(int))\n",
        "\n",
        "    df['punctuation'] = df['translation'].apply(_punctuation)\n",
        "\n",
        "\n",
        "PUNCTUATION = [',', '.', '\\'', '\\\"', '!', '?']\n",
        "\n",
        "\n",
        "def _punctuation(text: str) -> str:\n",
        "    n = len(text)\n",
        "    for i in range(n - 1, -1, -1):\n",
        "        if text[i] not in PUNCTUATION:\n",
        "            return text[i+1:] if i < n - 1 else ''\n",
        "    return text\n",
        "\n",
        "\n",
        "class Embeddings:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._embeddings: Dict[int, np.ndarray] = {}\n",
        "        self._default_vector: np.ndarray = np.zeros(256)\n",
        "        self._load_embeddings()\n",
        "\n",
        "    def get_vector(self, embeddingId: int):\n",
        "        return self._embeddings.get(embeddingId, self._default_vector)\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        VECTOR_FILE = '/kaggle/working/quran-neural-chunker/data/vectors.txt'\n",
        "        with open(VECTOR_FILE, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip().split()\n",
        "                embeddingId = int(line[0][:-1])\n",
        "                vector = np.array(list(map(float, line[2:])))\n",
        "                self._embeddings[embeddingId] = vector\n",
        "\n",
        "class Evaluator:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._expected_chunks = 0\n",
        "        self._output_chunks = 0\n",
        "        self._equivalent_chunks = 0\n",
        "\n",
        "    def compare(self, expected_chunks: List[Chunk], output_chunks: List[Chunk]):\n",
        "        expected_set = set(expected_chunks)\n",
        "        output_set = set(output_chunks)\n",
        "\n",
        "        self._expected_chunks += len(expected_set)\n",
        "        self._output_chunks += len(output_set)\n",
        "        self._equivalent_chunks += len(expected_set & output_set)\n",
        "\n",
        "    @property\n",
        "    def precision(self):\n",
        "        return 0 if self._output_chunks == 0 else self._equivalent_chunks / self._output_chunks\n",
        "\n",
        "    @property\n",
        "    def recall(self):\n",
        "        return 0 if self._expected_chunks == 0 else self._equivalent_chunks / self._expected_chunks\n",
        "\n",
        "    @property\n",
        "    def f1_score(self):\n",
        "        precision = self.precision\n",
        "        recall = self.recall\n",
        "        return 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    CHUNKS_FILE = '/kaggle/working/quran-neural-chunker/data/quranic-treebank-0.4-chunks.tsv'\n",
        "    return pd.read_csv(CHUNKS_FILE, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(input_size)\n",
        "        transformer_layers =nn.TransformerEncoderLayer(d_model=input_size, nhead=9, dim_feedforward=hidden_size, dropout=0.5)\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layers, num_layers)\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer(x)\n",
        "        out = self.fc(output)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 0:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        packed_output, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # unpack the output before passing through the linear layer\n",
        "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # manually pad the sequences to max_length\n",
        "        if output.size(1) < max_length:\n",
        "            output = nn.functional.pad(output, (0, 0, 0, max_length - output.size(1)))\n",
        "\n",
        "        out = self.fc(output)\n",
        "        return out\n",
        "\n",
        "\n",
        "class QuranDataset(Dataset):\n",
        "    def __init__(self, verses, labels):\n",
        "        self.verses = verses\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.verses)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        verse = self.verses[index]\n",
        "        label = self.labels[index]\n",
        "        length = len(verse)\n",
        "\n",
        "        # padding\n",
        "        if length < max_length:\n",
        "            verse.extend([[0]*len(verse[0])] * (max_length - length))\n",
        "            label.extend([0] * (max_length - length))\n",
        "\n",
        "        return torch.tensor(verse, dtype=torch.float32), torch.tensor(label), length\n",
        "\n",
        "\n",
        "def get_verses(df: DataFrame):\n",
        "    le = LabelEncoder()\n",
        "    df['encoded_punctuation'] = le.fit_transform(df['punctuation'])\n",
        "\n",
        "    word_vectors = Embeddings()\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        embedding_vector = word_vectors.get_vector(row['embedding_id'])\n",
        "        core_values = row[['token_number', 'pause_mark', 'irab_end', 'verse_end', 'encoded_punctuation']].values\n",
        "        full_vector = np.concatenate([core_values, embedding_vector]).tolist()\n",
        "        rows.append(full_vector + [row['chunk_end']])\n",
        "    X = pd.DataFrame(rows, columns=[f'feature_{i}' for i in range(261)]+['chunk_end'])\n",
        "\n",
        "    verses: List[List[int]] = []\n",
        "    labels: List[int] = []\n",
        "    verse_info: List[List[int]] = []\n",
        "\n",
        "    for _, group in df.groupby(['chapter_number', 'verse_number']):\n",
        "        group_df = X.loc[group.index]\n",
        "        verse = group_df[group_df.columns.difference(['chunk_end'])].values.tolist()\n",
        "        label = group_df['chunk_end'].tolist()\n",
        "\n",
        "        verses.append(verse)\n",
        "        labels.append(label)\n",
        "\n",
        "        verse_info_single = group[['chapter_number', 'verse_number', 'token_number']].values.tolist()\n",
        "        verse_info.append(verse_info_single)\n",
        "\n",
        "    temp_data = list(zip(verses, verse_info, labels))\n",
        "    train_temp, test_temp = train_test_split(temp_data, test_size=0.10, random_state=42)\n",
        "\n",
        "    train_verses, train_verse_info, train_labels = zip(*train_temp)\n",
        "    test_verses, test_verse_info, test_labels = zip(*test_temp)\n",
        "\n",
        "    return train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info\n",
        "\n",
        "\n",
        "def pack_labels(labels):\n",
        "    lengths = [len(label) for label in labels]\n",
        "    max_len = max(lengths)\n",
        "    labels_padded = [torch.cat([label, torch.zeros(max_len - len(label))]) for label in labels]\n",
        "    return torch.stack(labels_padded)\n",
        "\n",
        "\n",
        "def train_and_test_lstm():\n",
        "    df = load_data()\n",
        "    preprocess(df)\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    input_size = 261\n",
        "    hidden_size = 512\n",
        "    num_layers = 2\n",
        "    output_size = 2\n",
        "    num_epochs = 50\n",
        "    batch_size = 256\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info = get_verses(df)\n",
        "    print(f'Train verse count: {len(train_verses)}')\n",
        "    print(f'Test verse count: {len(test_verses)}')\n",
        "\n",
        "    training_data = QuranDataset(train_verses, train_labels)\n",
        "    testing_data = QuranDataset(test_verses, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = BiLSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    sched = CosineAnnealingLR(optimizer,eta_min=1e-10,T_max=len(train_loader)*num_epochs)\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (verses, labels, lengths) in tqdm(enumerate(train_loader)):\n",
        "            verses = verses.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            raw_outputs = model(verses, lengths)\n",
        "            labels = labels.view(-1)  # reshape labels to be a 1D tensor\n",
        "            loss = criterion(raw_outputs.view(-1, output_size), labels)\n",
        "\n",
        "            # backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sched.step()\n",
        "\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "        # test\n",
        "        model.eval()\n",
        "\n",
        "        expected_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "        output_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        with torch.no_grad():\n",
        "            for test_index in range(len(testing_data)):\n",
        "                verse, label, length = testing_data[test_index]\n",
        "                verse, label, length = verse.to(device).unsqueeze(0), label.to(device).unsqueeze(0), torch.tensor([length])\n",
        "\n",
        "                raw_output = model(verse, length)\n",
        "                _, predicted = torch.max(raw_output.data, 2)\n",
        "                predicted = predicted.cpu().numpy()\n",
        "\n",
        "                verse_info = test_verse_info[test_index]\n",
        "\n",
        "                for idx, token in enumerate(verse_info):\n",
        "                    expected_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': label.cpu().numpy()[0][idx]}, index=[0])\n",
        "                    expected_results_df = pd.concat([expected_results_df, expected_row])\n",
        "\n",
        "                    output_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': predicted[0][idx]}, index=[0])\n",
        "                    output_results_df = pd.concat([output_results_df, output_row])\n",
        "\n",
        "        # chunk-level evaluation\n",
        "        expected_chunks = get_chunks(expected_results_df)\n",
        "        output_chunks = get_chunks(output_results_df)\n",
        "        print(f'Expected: {len(expected_chunks)} chunks')\n",
        "        print(f'Output: {len(output_chunks)} chunks')\n",
        "\n",
        "        evaluator.compare(expected_chunks, output_chunks)\n",
        "        print(f'Precision: {evaluator.precision}')\n",
        "        print(f'Recall: {evaluator.recall}')\n",
        "        print(f'F1 score: {evaluator.f1_score}')\n",
        "        print()\n",
        "\n",
        "def train_and_test_transformer():\n",
        "    df = load_data()\n",
        "    preprocess(df)\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    input_size = 261\n",
        "    hidden_size = 261 * 9\n",
        "    num_layers = 3\n",
        "    output_size = 2\n",
        "    num_epochs = 50\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info = get_verses(df)\n",
        "    print(f'Train verse count: {len(train_verses)}')\n",
        "    print(f'Test verse count: {len(test_verses)}')\n",
        "\n",
        "    training_data = QuranDataset(train_verses, train_labels)\n",
        "    testing_data = QuranDataset(test_verses, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = TransformerModel(input_size, hidden_size, num_layers, output_size)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (verses, labels, lengths) in enumerate(train_loader):\n",
        "            verses = verses.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            raw_outputs = model(verses)\n",
        "            labels = labels.view(-1)  # reshape labels to be a 1D tensor\n",
        "            loss = criterion(raw_outputs.view(-1, output_size), labels)\n",
        "\n",
        "            # backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "        # test\n",
        "        model.eval()\n",
        "\n",
        "        expected_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "        output_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        with torch.no_grad():\n",
        "            for test_index in range(len(testing_data)):\n",
        "                verse, label, length = testing_data[test_index]\n",
        "                verse, label, length = verse.to(device).unsqueeze(0), label.to(device).unsqueeze(0), torch.tensor([length])\n",
        "\n",
        "                raw_output = model(verse)\n",
        "                _, predicted = torch.max(raw_output.data, 2)\n",
        "                predicted = predicted.cpu().numpy()\n",
        "\n",
        "                verse_info = test_verse_info[test_index]\n",
        "\n",
        "                for idx, token in enumerate(verse_info):\n",
        "                    expected_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': label.cpu().numpy()[0][idx]}, index=[0])\n",
        "                    expected_results_df = pd.concat([expected_results_df, expected_row])\n",
        "\n",
        "                    output_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': predicted[0][idx]}, index=[0])\n",
        "                    output_results_df = pd.concat([output_results_df, output_row])\n",
        "\n",
        "        # chunk-level evaluation\n",
        "        expected_chunks = get_chunks(expected_results_df)\n",
        "        output_chunks = get_chunks(output_results_df)\n",
        "        print(f'Expected: {len(expected_chunks)} chunks')\n",
        "        print(f'Output: {len(output_chunks)} chunks')\n",
        "\n",
        "        evaluator.compare(expected_chunks, output_chunks)\n",
        "        print(f'Precision: {evaluator.precision}')\n",
        "        print(f'Recall: {evaluator.recall}')\n",
        "        print(f'F1 score: {evaluator.f1_score}')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d6da",
      "metadata": {
        "papermill": {
          "duration": 0.012745,
          "end_time": "2023-07-05T19:14:13.999116",
          "exception": false,
          "start_time": "2023-07-05T19:14:13.986371",
          "status": "completed"
        },
        "tags": [],
        "id": "9714d6da"
      },
      "outputs": [],
      "source": [
        "train_and_test_transformer()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5346.935033,
      "end_time": "2023-07-05T19:14:15.650422",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-07-05T17:45:08.715389",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}