{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahmoudlimam/Personal-Projects/blob/main/treebank_chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6994d50",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:20.120991Z",
          "iopub.status.busy": "2023-07-05T17:45:20.120129Z",
          "iopub.status.idle": "2023-07-05T17:45:22.799307Z",
          "shell.execute_reply": "2023-07-05T17:45:22.797872Z"
        },
        "papermill": {
          "duration": 2.689035,
          "end_time": "2023-07-05T17:45:22.802168",
          "exception": false,
          "start_time": "2023-07-05T17:45:20.113133",
          "status": "completed"
        },
        "tags": [],
        "id": "f6994d50",
        "outputId": "bfe76cbe-b800-4a79-d881-a9922404bfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'quran-neural-chunker'...\r\n",
            "remote: Enumerating objects: 265, done.\u001b[K\r\n",
            "remote: Total 265 (delta 0), reused 0 (delta 0), pack-reused 265\u001b[K\r\n",
            "Receiving objects: 100% (265/265), 14.54 MiB | 21.54 MiB/s, done.\r\n",
            "Resolving deltas: 100% (144/144), done.\r\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kaisdukes/quran-neural-chunker.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa76012",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:22.811860Z",
          "iopub.status.busy": "2023-07-05T17:45:22.811462Z",
          "iopub.status.idle": "2023-07-05T17:45:23.893553Z",
          "shell.execute_reply": "2023-07-05T17:45:23.891967Z"
        },
        "papermill": {
          "duration": 1.090281,
          "end_time": "2023-07-05T17:45:23.896601",
          "exception": false,
          "start_time": "2023-07-05T17:45:22.806320",
          "status": "completed"
        },
        "tags": [],
        "id": "afa76012"
      },
      "outputs": [],
      "source": [
        "!cd /kaggle/working/quran-neural-chunker/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1b37f9",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:23.907016Z",
          "iopub.status.busy": "2023-07-05T17:45:23.906214Z",
          "iopub.status.idle": "2023-07-05T17:45:28.379894Z",
          "shell.execute_reply": "2023-07-05T17:45:28.378958Z"
        },
        "papermill": {
          "duration": 4.482031,
          "end_time": "2023-07-05T17:45:28.382418",
          "exception": false,
          "start_time": "2023-07-05T17:45:23.900387",
          "status": "completed"
        },
        "tags": [],
        "id": "6d1b37f9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from pandas import DataFrame\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "max_length = 128\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Location:\n",
        "    chapter_number: int\n",
        "    verse_number: int\n",
        "    token_number: int\n",
        "\n",
        "    def __str__(self):\n",
        "        parts = [str(self.chapter_number), str(self.verse_number)]\n",
        "        if self.token_number > 0:\n",
        "            parts.append(str(self.token_number))\n",
        "        return ':'.join(parts)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Chunk:\n",
        "    start: Location\n",
        "    end: Location\n",
        "\n",
        "\n",
        "def get_chunks(df: DataFrame):\n",
        "    chunks: List[Chunk] = []\n",
        "    start: Location = None\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        loc = Location(row['chapter_number'], row['verse_number'], row['token_number'])\n",
        "        if start is None:\n",
        "            start = loc\n",
        "        if row['chunk_end'] == 1:\n",
        "            end = loc\n",
        "            chunk = Chunk(start, end)\n",
        "            chunks.append(chunk)\n",
        "            start = None\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def preprocess(df: DataFrame):\n",
        "    df['verse_end'] = (\n",
        "        (df.groupby(['chapter_number', 'verse_number']).token_number.transform(max) == df.token_number)\n",
        "        .astype(int))\n",
        "\n",
        "    df['punctuation'] = df['translation'].apply(_punctuation)\n",
        "\n",
        "\n",
        "PUNCTUATION = [',', '.', '\\'', '\\\"', '!', '?']\n",
        "\n",
        "\n",
        "def _punctuation(text: str) -> str:\n",
        "    n = len(text)\n",
        "    for i in range(n - 1, -1, -1):\n",
        "        if text[i] not in PUNCTUATION:\n",
        "            return text[i+1:] if i < n - 1 else ''\n",
        "    return text\n",
        "\n",
        "\n",
        "class Embeddings:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._embeddings: Dict[int, np.ndarray] = {}\n",
        "        self._default_vector: np.ndarray = np.zeros(256)\n",
        "        self._load_embeddings()\n",
        "\n",
        "    def get_vector(self, embeddingId: int):\n",
        "        return self._embeddings.get(embeddingId, self._default_vector)\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        VECTOR_FILE = '/kaggle/working/quran-neural-chunker/data/vectors.txt'\n",
        "        with open(VECTOR_FILE, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip().split()\n",
        "                embeddingId = int(line[0][:-1])\n",
        "                vector = np.array(list(map(float, line[2:])))\n",
        "                self._embeddings[embeddingId] = vector\n",
        "\n",
        "class Evaluator:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._expected_chunks = 0\n",
        "        self._output_chunks = 0\n",
        "        self._equivalent_chunks = 0\n",
        "\n",
        "    def compare(self, expected_chunks: List[Chunk], output_chunks: List[Chunk]):\n",
        "        expected_set = set(expected_chunks)\n",
        "        output_set = set(output_chunks)\n",
        "\n",
        "        self._expected_chunks += len(expected_set)\n",
        "        self._output_chunks += len(output_set)\n",
        "        self._equivalent_chunks += len(expected_set & output_set)\n",
        "\n",
        "    @property\n",
        "    def precision(self):\n",
        "        return 0 if self._output_chunks == 0 else self._equivalent_chunks / self._output_chunks\n",
        "\n",
        "    @property\n",
        "    def recall(self):\n",
        "        return 0 if self._expected_chunks == 0 else self._equivalent_chunks / self._expected_chunks\n",
        "\n",
        "    @property\n",
        "    def f1_score(self):\n",
        "        precision = self.precision\n",
        "        recall = self.recall\n",
        "        return 0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    CHUNKS_FILE = '/kaggle/working/quran-neural-chunker/data/quranic-treebank-0.4-chunks.tsv'\n",
        "    return pd.read_csv(CHUNKS_FILE, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(input_size)\n",
        "        transformer_layers =nn.TransformerEncoderLayer(d_model=input_size, nhead=9, dim_feedforward=hidden_size, dropout=0.5)\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layers, num_layers)\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_encoder(x)\n",
        "        output = self.transformer(x)\n",
        "        out = self.fc(output)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 0:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        packed_output, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # unpack the output before passing through the linear layer\n",
        "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # manually pad the sequences to max_length\n",
        "        if output.size(1) < max_length:\n",
        "            output = nn.functional.pad(output, (0, 0, 0, max_length - output.size(1)))\n",
        "\n",
        "        out = self.fc(output)\n",
        "        return out\n",
        "\n",
        "\n",
        "class QuranDataset(Dataset):\n",
        "    def __init__(self, verses, labels):\n",
        "        self.verses = verses\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.verses)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        verse = self.verses[index]\n",
        "        label = self.labels[index]\n",
        "        length = len(verse)\n",
        "\n",
        "        # padding\n",
        "        if length < max_length:\n",
        "            verse.extend([[0]*len(verse[0])] * (max_length - length))\n",
        "            label.extend([0] * (max_length - length))\n",
        "\n",
        "        return torch.tensor(verse, dtype=torch.float32), torch.tensor(label), length\n",
        "\n",
        "\n",
        "def get_verses(df: DataFrame):\n",
        "    le = LabelEncoder()\n",
        "    df['encoded_punctuation'] = le.fit_transform(df['punctuation'])\n",
        "\n",
        "    word_vectors = Embeddings()\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        embedding_vector = word_vectors.get_vector(row['embedding_id'])\n",
        "        core_values = row[['token_number', 'pause_mark', 'irab_end', 'verse_end', 'encoded_punctuation']].values\n",
        "        full_vector = np.concatenate([core_values, embedding_vector]).tolist()\n",
        "        rows.append(full_vector + [row['chunk_end']])\n",
        "    X = pd.DataFrame(rows, columns=[f'feature_{i}' for i in range(261)]+['chunk_end'])\n",
        "\n",
        "    verses: List[List[int]] = []\n",
        "    labels: List[int] = []\n",
        "    verse_info: List[List[int]] = []\n",
        "\n",
        "    for _, group in df.groupby(['chapter_number', 'verse_number']):\n",
        "        group_df = X.loc[group.index]\n",
        "        verse = group_df[group_df.columns.difference(['chunk_end'])].values.tolist()\n",
        "        label = group_df['chunk_end'].tolist()\n",
        "\n",
        "        verses.append(verse)\n",
        "        labels.append(label)\n",
        "\n",
        "        verse_info_single = group[['chapter_number', 'verse_number', 'token_number']].values.tolist()\n",
        "        verse_info.append(verse_info_single)\n",
        "\n",
        "    temp_data = list(zip(verses, verse_info, labels))\n",
        "    train_temp, test_temp = train_test_split(temp_data, test_size=0.10, random_state=42)\n",
        "\n",
        "    train_verses, train_verse_info, train_labels = zip(*train_temp)\n",
        "    test_verses, test_verse_info, test_labels = zip(*test_temp)\n",
        "\n",
        "    return train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info\n",
        "\n",
        "\n",
        "def pack_labels(labels):\n",
        "    lengths = [len(label) for label in labels]\n",
        "    max_len = max(lengths)\n",
        "    labels_padded = [torch.cat([label, torch.zeros(max_len - len(label))]) for label in labels]\n",
        "    return torch.stack(labels_padded)\n",
        "\n",
        "\n",
        "def train_and_test_lstm():\n",
        "    df = load_data()\n",
        "    preprocess(df)\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    input_size = 261\n",
        "    hidden_size = 512\n",
        "    num_layers = 2\n",
        "    output_size = 2\n",
        "    num_epochs = 50\n",
        "    batch_size = 256\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info = get_verses(df)\n",
        "    print(f'Train verse count: {len(train_verses)}')\n",
        "    print(f'Test verse count: {len(test_verses)}')\n",
        "\n",
        "    training_data = QuranDataset(train_verses, train_labels)\n",
        "    testing_data = QuranDataset(test_verses, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = BiLSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    sched = CosineAnnealingLR(optimizer,eta_min=1e-10,T_max=len(train_loader)*num_epochs)\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (verses, labels, lengths) in tqdm(enumerate(train_loader)):\n",
        "            verses = verses.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            raw_outputs = model(verses, lengths)\n",
        "            labels = labels.view(-1)  # reshape labels to be a 1D tensor\n",
        "            loss = criterion(raw_outputs.view(-1, output_size), labels)\n",
        "\n",
        "            # backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sched.step()\n",
        "\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "        # test\n",
        "        model.eval()\n",
        "\n",
        "        expected_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "        output_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        with torch.no_grad():\n",
        "            for test_index in range(len(testing_data)):\n",
        "                verse, label, length = testing_data[test_index]\n",
        "                verse, label, length = verse.to(device).unsqueeze(0), label.to(device).unsqueeze(0), torch.tensor([length])\n",
        "\n",
        "                raw_output = model(verse, length)\n",
        "                _, predicted = torch.max(raw_output.data, 2)\n",
        "                predicted = predicted.cpu().numpy()\n",
        "\n",
        "                verse_info = test_verse_info[test_index]\n",
        "\n",
        "                for idx, token in enumerate(verse_info):\n",
        "                    expected_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': label.cpu().numpy()[0][idx]}, index=[0])\n",
        "                    expected_results_df = pd.concat([expected_results_df, expected_row])\n",
        "\n",
        "                    output_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': predicted[0][idx]}, index=[0])\n",
        "                    output_results_df = pd.concat([output_results_df, output_row])\n",
        "\n",
        "        # chunk-level evaluation\n",
        "        expected_chunks = get_chunks(expected_results_df)\n",
        "        output_chunks = get_chunks(output_results_df)\n",
        "        print(f'Expected: {len(expected_chunks)} chunks')\n",
        "        print(f'Output: {len(output_chunks)} chunks')\n",
        "\n",
        "        evaluator.compare(expected_chunks, output_chunks)\n",
        "        print(f'Precision: {evaluator.precision}')\n",
        "        print(f'Recall: {evaluator.recall}')\n",
        "        print(f'F1 score: {evaluator.f1_score}')\n",
        "        print()\n",
        "\n",
        "def train_and_test_transformer():\n",
        "    df = load_data()\n",
        "    preprocess(df)\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    input_size = 261\n",
        "    hidden_size = 261 * 9\n",
        "    num_layers = 3\n",
        "    output_size = 2\n",
        "    num_epochs = 50\n",
        "    batch_size = 64\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    train_verses, test_verses, train_labels, test_labels, train_verse_info, test_verse_info = get_verses(df)\n",
        "    print(f'Train verse count: {len(train_verses)}')\n",
        "    print(f'Test verse count: {len(test_verses)}')\n",
        "\n",
        "    training_data = QuranDataset(train_verses, train_labels)\n",
        "    testing_data = QuranDataset(test_verses, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = TransformerModel(input_size, hidden_size, num_layers, output_size)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (verses, labels, lengths) in enumerate(train_loader):\n",
        "            verses = verses.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            raw_outputs = model(verses)\n",
        "            labels = labels.view(-1)  # reshape labels to be a 1D tensor\n",
        "            loss = criterion(raw_outputs.view(-1, output_size), labels)\n",
        "\n",
        "            # backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "        # test\n",
        "        model.eval()\n",
        "\n",
        "        expected_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "        output_results_df = DataFrame(columns=['chapter_number', 'verse_number', 'token_number', 'chunk_end'])\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        with torch.no_grad():\n",
        "            for test_index in range(len(testing_data)):\n",
        "                verse, label, length = testing_data[test_index]\n",
        "                verse, label, length = verse.to(device).unsqueeze(0), label.to(device).unsqueeze(0), torch.tensor([length])\n",
        "\n",
        "                raw_output = model(verse)\n",
        "                _, predicted = torch.max(raw_output.data, 2)\n",
        "                predicted = predicted.cpu().numpy()\n",
        "\n",
        "                verse_info = test_verse_info[test_index]\n",
        "\n",
        "                for idx, token in enumerate(verse_info):\n",
        "                    expected_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': label.cpu().numpy()[0][idx]}, index=[0])\n",
        "                    expected_results_df = pd.concat([expected_results_df, expected_row])\n",
        "\n",
        "                    output_row = DataFrame({\n",
        "                        'chapter_number': token[0],\n",
        "                        'verse_number': token[1],\n",
        "                        'token_number': token[2],\n",
        "                        'chunk_end': predicted[0][idx]}, index=[0])\n",
        "                    output_results_df = pd.concat([output_results_df, output_row])\n",
        "\n",
        "        # chunk-level evaluation\n",
        "        expected_chunks = get_chunks(expected_results_df)\n",
        "        output_chunks = get_chunks(output_results_df)\n",
        "        print(f'Expected: {len(expected_chunks)} chunks')\n",
        "        print(f'Output: {len(output_chunks)} chunks')\n",
        "\n",
        "        evaluator.compare(expected_chunks, output_chunks)\n",
        "        print(f'Precision: {evaluator.precision}')\n",
        "        print(f'Recall: {evaluator.recall}')\n",
        "        print(f'F1 score: {evaluator.f1_score}')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e801bc79",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-05T17:45:28.391642Z",
          "iopub.status.busy": "2023-07-05T17:45:28.391028Z",
          "iopub.status.idle": "2023-07-05T19:14:13.970021Z",
          "shell.execute_reply": "2023-07-05T19:14:13.968462Z"
        },
        "papermill": {
          "duration": 5325.586959,
          "end_time": "2023-07-05T19:14:13.973214",
          "exception": false,
          "start_time": "2023-07-05T17:45:28.386255",
          "status": "completed"
        },
        "tags": [],
        "id": "e801bc79",
        "outputId": "bad09077-917c-476e-f371-2d7dab4f9406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train verse count: 2192\n",
            "Test verse count: 244\n",
            "Epoch [1/50], Loss: 0.0596\n",
            "Expected: 768 chunks\n",
            "Output: 639 chunks\n",
            "Precision: 0.04381846635367762\n",
            "Recall: 0.036458333333333336\n",
            "F1 score: 0.03980099502487563\n",
            "\n",
            "Epoch [2/50], Loss: 0.0342\n",
            "Expected: 768 chunks\n",
            "Output: 1722 chunks\n",
            "Precision: 0.11149825783972125\n",
            "Recall: 0.25\n",
            "F1 score: 0.15421686746987953\n",
            "\n",
            "Epoch [3/50], Loss: 0.0413\n",
            "Expected: 768 chunks\n",
            "Output: 1769 chunks\n",
            "Precision: 0.11758055398530243\n",
            "Recall: 0.2708333333333333\n",
            "F1 score: 0.16397319668900273\n",
            "\n",
            "Epoch [4/50], Loss: 0.0380\n",
            "Expected: 768 chunks\n",
            "Output: 1853 chunks\n",
            "Precision: 0.10361575822989746\n",
            "Recall: 0.25\n",
            "F1 score: 0.14650896604349486\n",
            "\n",
            "Epoch [5/50], Loss: 0.0226\n",
            "Expected: 768 chunks\n",
            "Output: 1757 chunks\n",
            "Precision: 0.12122936824132043\n",
            "Recall: 0.27734375\n",
            "F1 score: 0.1687128712871287\n",
            "\n",
            "Epoch [6/50], Loss: 0.0689\n",
            "Expected: 768 chunks\n",
            "Output: 1272 chunks\n",
            "Precision: 0.23427672955974843\n",
            "Recall: 0.3880208333333333\n",
            "F1 score: 0.29215686274509806\n",
            "\n",
            "Epoch [7/50], Loss: 0.0771\n",
            "Expected: 768 chunks\n",
            "Output: 460 chunks\n",
            "Precision: 0.5043478260869565\n",
            "Recall: 0.3020833333333333\n",
            "F1 score: 0.3778501628664495\n",
            "\n",
            "Epoch [8/50], Loss: 0.0239\n",
            "Expected: 768 chunks\n",
            "Output: 599 chunks\n",
            "Precision: 0.5141903171953256\n",
            "Recall: 0.4010416666666667\n",
            "F1 score: 0.4506217995610827\n",
            "\n",
            "Epoch [9/50], Loss: 0.0180\n",
            "Expected: 768 chunks\n",
            "Output: 500 chunks\n",
            "Precision: 0.496\n",
            "Recall: 0.3229166666666667\n",
            "F1 score: 0.3911671924290221\n",
            "\n",
            "Epoch [10/50], Loss: 0.0221\n",
            "Expected: 768 chunks\n",
            "Output: 580 chunks\n",
            "Precision: 0.5206896551724138\n",
            "Recall: 0.3932291666666667\n",
            "F1 score: 0.44807121661721067\n",
            "\n",
            "Epoch [11/50], Loss: 0.0158\n",
            "Expected: 768 chunks\n",
            "Output: 718 chunks\n",
            "Precision: 0.5306406685236769\n",
            "Recall: 0.49609375\n",
            "F1 score: 0.5127860026917901\n",
            "\n",
            "Epoch [12/50], Loss: 0.0208\n",
            "Expected: 768 chunks\n",
            "Output: 623 chunks\n",
            "Precision: 0.5617977528089888\n",
            "Recall: 0.4557291666666667\n",
            "F1 score: 0.503235082674335\n",
            "\n",
            "Epoch [13/50], Loss: 0.0104\n",
            "Expected: 768 chunks\n",
            "Output: 649 chunks\n",
            "Precision: 0.5546995377503852\n",
            "Recall: 0.46875\n",
            "F1 score: 0.5081157374735356\n",
            "\n",
            "Epoch [14/50], Loss: 0.0209\n",
            "Expected: 768 chunks\n",
            "Output: 732 chunks\n",
            "Precision: 0.5218579234972678\n",
            "Recall: 0.4973958333333333\n",
            "F1 score: 0.5093333333333334\n",
            "\n",
            "Epoch [15/50], Loss: 0.0156\n",
            "Expected: 768 chunks\n",
            "Output: 729 chunks\n",
            "Precision: 0.5308641975308642\n",
            "Recall: 0.50390625\n",
            "F1 score: 0.5170340681362725\n",
            "\n",
            "Epoch [16/50], Loss: 0.0290\n",
            "Expected: 768 chunks\n",
            "Output: 779 chunks\n",
            "Precision: 0.5006418485237484\n",
            "Recall: 0.5078125\n",
            "F1 score: 0.5042016806722689\n",
            "\n",
            "Epoch [17/50], Loss: 0.0227\n",
            "Expected: 768 chunks\n",
            "Output: 764 chunks\n",
            "Precision: 0.5130890052356021\n",
            "Recall: 0.5104166666666666\n",
            "F1 score: 0.5117493472584856\n",
            "\n",
            "Epoch [18/50], Loss: 0.0157\n",
            "Expected: 768 chunks\n",
            "Output: 752 chunks\n",
            "Precision: 0.5279255319148937\n",
            "Recall: 0.5169270833333334\n",
            "F1 score: 0.5223684210526316\n",
            "\n",
            "Epoch [19/50], Loss: 0.0227\n",
            "Expected: 768 chunks\n",
            "Output: 797 chunks\n",
            "Precision: 0.5043914680050188\n",
            "Recall: 0.5234375\n",
            "F1 score: 0.513738019169329\n",
            "\n",
            "Epoch [20/50], Loss: 0.0176\n",
            "Expected: 768 chunks\n",
            "Output: 666 chunks\n",
            "Precision: 0.5735735735735735\n",
            "Recall: 0.4973958333333333\n",
            "F1 score: 0.5327754532775453\n",
            "\n",
            "Epoch [21/50], Loss: 0.0142\n",
            "Expected: 768 chunks\n",
            "Output: 661 chunks\n",
            "Precision: 0.5854765506807866\n",
            "Recall: 0.50390625\n",
            "F1 score: 0.5416375087473757\n",
            "\n",
            "Epoch [22/50], Loss: 0.0217\n",
            "Expected: 768 chunks\n",
            "Output: 677 chunks\n",
            "Precision: 0.5834564254062038\n",
            "Recall: 0.5143229166666666\n",
            "F1 score: 0.546712802768166\n",
            "\n",
            "Epoch [23/50], Loss: 0.0176\n",
            "Expected: 768 chunks\n",
            "Output: 706 chunks\n",
            "Precision: 0.5679886685552408\n",
            "Recall: 0.5221354166666666\n",
            "F1 score: 0.5440976933514247\n",
            "\n",
            "Epoch [24/50], Loss: 0.0255\n",
            "Expected: 768 chunks\n",
            "Output: 688 chunks\n",
            "Precision: 0.5828488372093024\n",
            "Recall: 0.5221354166666666\n",
            "F1 score: 0.5508241758241758\n",
            "\n",
            "Epoch [25/50], Loss: 0.0161\n",
            "Expected: 768 chunks\n",
            "Output: 699 chunks\n",
            "Precision: 0.5851216022889842\n",
            "Recall: 0.5325520833333334\n",
            "F1 score: 0.5576005453306068\n",
            "\n",
            "Epoch [26/50], Loss: 0.0074\n",
            "Expected: 768 chunks\n",
            "Output: 586 chunks\n",
            "Precision: 0.6040955631399317\n",
            "Recall: 0.4609375\n",
            "F1 score: 0.5228951255539144\n",
            "\n",
            "Epoch [27/50], Loss: 0.0163\n",
            "Expected: 768 chunks\n",
            "Output: 685 chunks\n",
            "Precision: 0.5547445255474452\n",
            "Recall: 0.4947916666666667\n",
            "F1 score: 0.5230557467309015\n",
            "\n",
            "Epoch [28/50], Loss: 0.0141\n",
            "Expected: 768 chunks\n",
            "Output: 628 chunks\n",
            "Precision: 0.6226114649681529\n",
            "Recall: 0.5091145833333334\n",
            "F1 score: 0.5601719197707736\n",
            "\n",
            "Epoch [29/50], Loss: 0.0241\n",
            "Expected: 768 chunks\n",
            "Output: 610 chunks\n",
            "Precision: 0.6147540983606558\n",
            "Recall: 0.48828125\n",
            "F1 score: 0.544267053701016\n",
            "\n",
            "Epoch [30/50], Loss: 0.0145\n",
            "Expected: 768 chunks\n",
            "Output: 574 chunks\n",
            "Precision: 0.6010452961672473\n",
            "Recall: 0.44921875\n",
            "F1 score: 0.5141579731743666\n",
            "\n",
            "Epoch [31/50], Loss: 0.0127\n",
            "Expected: 768 chunks\n",
            "Output: 588 chunks\n",
            "Precision: 0.5952380952380952\n",
            "Recall: 0.4557291666666667\n",
            "F1 score: 0.5162241887905605\n",
            "\n",
            "Epoch [32/50], Loss: 0.0181\n",
            "Expected: 768 chunks\n",
            "Output: 594 chunks\n",
            "Precision: 0.5993265993265994\n",
            "Recall: 0.4635416666666667\n",
            "F1 score: 0.5227606461086638\n",
            "\n",
            "Epoch [33/50], Loss: 0.0180\n",
            "Expected: 768 chunks\n",
            "Output: 542 chunks\n",
            "Precision: 0.584870848708487\n",
            "Recall: 0.4127604166666667\n",
            "F1 score: 0.4839694656488549\n",
            "\n",
            "Epoch [34/50], Loss: 0.0128\n",
            "Expected: 768 chunks\n",
            "Output: 579 chunks\n",
            "Precision: 0.5924006908462867\n",
            "Recall: 0.4466145833333333\n",
            "F1 score: 0.5092798812175204\n",
            "\n",
            "Epoch [35/50], Loss: 0.0126\n",
            "Expected: 768 chunks\n",
            "Output: 571 chunks\n",
            "Precision: 0.574430823117338\n",
            "Recall: 0.4270833333333333\n",
            "F1 score: 0.48991784914115005\n",
            "\n",
            "Epoch [36/50], Loss: 0.0114\n",
            "Expected: 768 chunks\n",
            "Output: 582 chunks\n",
            "Precision: 0.5807560137457045\n",
            "Recall: 0.4401041666666667\n",
            "F1 score: 0.5007407407407407\n",
            "\n",
            "Epoch [37/50], Loss: 0.0127\n",
            "Expected: 768 chunks\n",
            "Output: 594 chunks\n",
            "Precision: 0.5723905723905723\n",
            "Recall: 0.4427083333333333\n",
            "F1 score: 0.4992657856093979\n",
            "\n",
            "Epoch [38/50], Loss: 0.0175\n",
            "Expected: 768 chunks\n",
            "Output: 559 chunks\n",
            "Precision: 0.5724508050089445\n",
            "Recall: 0.4166666666666667\n",
            "F1 score: 0.48229088168801804\n",
            "\n",
            "Epoch [39/50], Loss: 0.0103\n",
            "Expected: 768 chunks\n",
            "Output: 625 chunks\n",
            "Precision: 0.5696\n",
            "Recall: 0.4635416666666667\n",
            "F1 score: 0.511127063890883\n",
            "\n",
            "Epoch [40/50], Loss: 0.0117\n",
            "Expected: 768 chunks\n",
            "Output: 643 chunks\n",
            "Precision: 0.5427682737169518\n",
            "Recall: 0.4544270833333333\n",
            "F1 score: 0.4946846208362863\n",
            "\n",
            "Epoch [41/50], Loss: 0.0165\n",
            "Expected: 768 chunks\n",
            "Output: 583 chunks\n",
            "Precision: 0.5849056603773585\n",
            "Recall: 0.4440104166666667\n",
            "F1 score: 0.5048112509252406\n",
            "\n",
            "Epoch [42/50], Loss: 0.0139\n",
            "Expected: 768 chunks\n",
            "Output: 567 chunks\n",
            "Precision: 0.5343915343915344\n",
            "Recall: 0.39453125\n",
            "F1 score: 0.45393258426966293\n",
            "\n",
            "Epoch [43/50], Loss: 0.0119\n",
            "Expected: 768 chunks\n",
            "Output: 448 chunks\n",
            "Precision: 0.5223214285714286\n",
            "Recall: 0.3046875\n",
            "F1 score: 0.3848684210526316\n",
            "\n",
            "Epoch [44/50], Loss: 0.0189\n",
            "Expected: 768 chunks\n",
            "Output: 535 chunks\n",
            "Precision: 0.5588785046728972\n",
            "Recall: 0.3893229166666667\n",
            "F1 score: 0.4589409056024559\n",
            "\n",
            "Epoch [45/50], Loss: 0.0131\n",
            "Expected: 768 chunks\n",
            "Output: 608 chunks\n",
            "Precision: 0.5493421052631579\n",
            "Recall: 0.4348958333333333\n",
            "F1 score: 0.48546511627906974\n",
            "\n",
            "Epoch [46/50], Loss: 0.0097\n",
            "Expected: 768 chunks\n",
            "Output: 586 chunks\n",
            "Precision: 0.5494880546075085\n",
            "Recall: 0.4192708333333333\n",
            "F1 score: 0.4756277695716396\n",
            "\n",
            "Epoch [47/50], Loss: 0.0110\n",
            "Expected: 768 chunks\n",
            "Output: 597 chunks\n",
            "Precision: 0.5527638190954773\n",
            "Recall: 0.4296875\n",
            "F1 score: 0.48351648351648346\n",
            "\n",
            "Epoch [48/50], Loss: 0.0155\n",
            "Expected: 768 chunks\n",
            "Output: 635 chunks\n",
            "Precision: 0.552755905511811\n",
            "Recall: 0.45703125\n",
            "F1 score: 0.5003563791874555\n",
            "\n",
            "Epoch [49/50], Loss: 0.0144\n",
            "Expected: 768 chunks\n",
            "Output: 545 chunks\n",
            "Precision: 0.5504587155963303\n",
            "Recall: 0.390625\n",
            "F1 score: 0.456968773800457\n",
            "\n",
            "Epoch [50/50], Loss: 0.0119\n",
            "Expected: 768 chunks\n",
            "Output: 584 chunks\n",
            "Precision: 0.5256849315068494\n",
            "Recall: 0.3997395833333333\n",
            "F1 score: 0.4541420118343195\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_and_test_transformer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d6da",
      "metadata": {
        "papermill": {
          "duration": 0.012745,
          "end_time": "2023-07-05T19:14:13.999116",
          "exception": false,
          "start_time": "2023-07-05T19:14:13.986371",
          "status": "completed"
        },
        "tags": [],
        "id": "9714d6da"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5346.935033,
      "end_time": "2023-07-05T19:14:15.650422",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-07-05T17:45:08.715389",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}